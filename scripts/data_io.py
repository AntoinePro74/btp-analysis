"""
Module de gestion des entrÃ©es/sorties de donnÃ©es
"""
import pandas as pd
import os
import json
from datetime import datetime


# ========================================
# FONCTIONS DONNÃ‰ES BRUTES (raw)
# ========================================

def load_raw_data(code_ape, data_dir="data/raw"):
    """
    Charge les donnÃ©es brutes depuis un fichier Parquet
    
    Args:
        code_ape (str): Code APE
        data_dir (str): RÃ©pertoire des donnÃ©es brutes
    
    Returns:
        pd.DataFrame: DataFrame des donnÃ©es brutes
    """
    filepath = f"{data_dir}/raw_entreprises_{code_ape}.parquet"
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"âŒ Fichier introuvable : {filepath}")
    
    df = pd.read_parquet(filepath)
    print(f"ğŸ“‚ ChargÃ© : {filepath} ({len(df)} lignes, {len(df.columns)} colonnes)")
    return df


def save_raw_data(entreprises, code_ape, output_dir="data/raw"):
    """
    Sauvegarde les donnÃ©es brutes finales + mÃ©tadonnÃ©es
    
    Args:
        entreprises (list): Liste des Ã©tablissements (dict)
        code_ape (str): Code APE
        output_dir (str): RÃ©pertoire de sortie
    
    Returns:
        str: Chemin du fichier crÃ©Ã©
    """
    os.makedirs(output_dir, exist_ok=True)
    df = pd.json_normalize(entreprises)
    filepath = f"{output_dir}/raw_entreprises_{code_ape}.parquet"
    df.to_parquet(filepath, engine="pyarrow", index=False)
    
    # Sauvegarder mÃ©tadonnÃ©es comme "completed"
    save_metadata(code_ape, "completed", len(entreprises), output_dir)
    
    # Supprimer le checkpoint si existe
    checkpoint_path = f"{output_dir}/checkpoint_{code_ape}.parquet"
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
    
    print(f"ğŸ’¾ {code_ape} : DonnÃ©es sauvegardÃ©es - {filepath}")
    return filepath


def save_checkpoint(entreprises, code_ape, output_dir="data/raw"):
    """
    Sauvegarde un checkpoint (extraction partielle)
    
    Args:
        entreprises (list): Liste des Ã©tablissements (dict)
        code_ape (str): Code APE
        output_dir (str): RÃ©pertoire de sortie
    """
    os.makedirs(output_dir, exist_ok=True)
    df = pd.json_normalize(entreprises)
    filepath = f"{output_dir}/checkpoint_{code_ape}.parquet"
    df.to_parquet(filepath, engine="pyarrow", index=False)
    print(f"ğŸ’¾ Checkpoint : {len(entreprises)} Ã©tablissements sauvegardÃ©s")


# ========================================
# FONCTIONS MÃ‰TADONNÃ‰ES
# ========================================

def get_metadata_path(code_ape, output_dir="data/raw"):
    """Retourne le chemin du fichier de mÃ©tadonnÃ©es"""
    return f"{output_dir}/.metadata_{code_ape}.json"


def save_metadata(code_ape, status, nb_etablissements, output_dir="data/raw"):
    """
    Sauvegarde les mÃ©tadonnÃ©es d'extraction
    
    Args:
        code_ape (str): Code APE
        status (str): 'completed' ou 'partial'
        nb_etablissements (int): Nombre d'Ã©tablissements extraits
        output_dir (str): RÃ©pertoire de sortie
    """
    metadata = {
        "code_ape": code_ape,
        "extraction_date": datetime.now().isoformat(),
        "status": status,
        "nb_etablissements": nb_etablissements
    }
    
    filepath = get_metadata_path(code_ape, output_dir)
    with open(filepath, 'w') as f:
        json.dump(metadata, f, indent=2)


def load_metadata(code_ape, output_dir="data/raw"):
    """
    Charge les mÃ©tadonnÃ©es d'extraction
    
    Args:
        code_ape (str): Code APE
        output_dir (str): RÃ©pertoire de sortie
    
    Returns:
        dict: MÃ©tadonnÃ©es ou None si absent
    """
    filepath = get_metadata_path(code_ape, output_dir)
    
    if not os.path.exists(filepath):
        return None
    
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except:
        return None


# ========================================
# FONCTIONS DONNÃ‰ES TRAITÃ‰ES (processed/final)
# ========================================

def save_split_data(df_siret, df_siren, df_full, code_ape, output_dir="data/processed"):
    """
    Sauvegarde les donnÃ©es en 3 formats : SIRET, SIREN, et FULL
    
    Args:
        df_siret (pd.DataFrame): DataFrame des Ã©tablissements (grain SIRET)
        df_siren (pd.DataFrame): DataFrame des unitÃ©s lÃ©gales (grain SIREN)
        df_full (pd.DataFrame): DataFrame complet (table plate)
        code_ape (str): Code APE
        output_dir (str): RÃ©pertoire de sortie
    
    Returns:
        dict: Chemins des fichiers crÃ©Ã©s
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Sauvegarde SIRET
    path_siret = f"{output_dir}/siret_{code_ape}.parquet"
    df_siret.to_parquet(path_siret, engine="pyarrow", index=False)
    print(f"   ğŸ“„ SIRET : {len(df_siret)} lignes â†’ {path_siret}")
    
    # 2. Sauvegarde SIREN
    path_siren = f"{output_dir}/siren_{code_ape}.parquet"
    df_siren.to_parquet(path_siren, engine="pyarrow", index=False)
    print(f"   ğŸ“„ SIREN : {len(df_siren)} lignes â†’ {path_siren}")
    
    # 3. Sauvegarde FULL
    path_full = f"{output_dir}/full_{code_ape}.parquet"
    df_full.to_parquet(path_full, engine="pyarrow", index=False)
    print(f"   ğŸ“„ FULL  : {len(df_full)} lignes â†’ {path_full}")
    
    return {
        "siret": path_siret,
        "siren": path_siren,
        "full": path_full
    }


def load_processed_data(code_ape, table_type="full", data_dir="data/processed"):
    """
    Charge les donnÃ©es traitÃ©es depuis un fichier Parquet
    
    Args:
        code_ape (str): Code APE
        table_type (str): Type de table ('siret', 'siren', ou 'full')
        data_dir (str): RÃ©pertoire des donnÃ©es traitÃ©es
    
    Returns:
        pd.DataFrame: DataFrame des donnÃ©es traitÃ©es
    """
    if table_type not in ["siret", "siren", "full"]:
        raise ValueError(f"âŒ table_type doit Ãªtre 'siret', 'siren', ou 'full', pas '{table_type}'")
    
    filepath = f"{data_dir}/{table_type}_{code_ape}.parquet"
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"âŒ Fichier introuvable : {filepath}")
    
    df = pd.read_parquet(filepath)
    print(f"ğŸ“‚ ChargÃ© : {filepath} ({len(df)} lignes, {len(df.columns)} colonnes)")
    return df


# === BLOC DE TEST ===
if __name__ == "__main__":
    """
    Test du module I/O
    Usage: python scripts/data_io.py
    """
    print("ğŸ§ª Test du module data_io\n")
    
    CODE_APE_TEST = "43.22A"
    
    # Test 1: Chargement donnÃ©es brutes
    print("ğŸ“‚ Test chargement donnÃ©es brutes...")
    df_raw = load_raw_data(CODE_APE_TEST)
    print(f"   âœ… {len(df_raw)} lignes chargÃ©es\n")
    
    # Test 2: MÃ©tadonnÃ©es
    print("ğŸ“„ Test mÃ©tadonnÃ©es...")
    metadata = load_metadata(CODE_APE_TEST)
    if metadata:
        print(f"   âœ… MÃ©tadonnÃ©es chargÃ©es : {metadata['status']}, {metadata['nb_etablissements']} Ã©tab., {metadata['extraction_date']}\n")
    else:
        print(f"   âš ï¸ Pas de mÃ©tadonnÃ©es trouvÃ©es\n")
    
    # Test 3: Chargement donnÃ©es traitÃ©es
    print("ğŸ“‚ Test chargement donnÃ©es traitÃ©es...")
    try:
        df_siret = load_processed_data(CODE_APE_TEST, "siret")
        df_siren = load_processed_data(CODE_APE_TEST, "siren")
        df_full = load_processed_data(CODE_APE_TEST, "full")
        print(f"   âœ… 3 tables chargÃ©es avec succÃ¨s\n")
    except FileNotFoundError as e:
        print(f"   âš ï¸ {e}\n")
    
    print("ğŸ‰ Tests I/O rÃ©ussis !")
